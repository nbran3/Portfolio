Hello, this is a project I did entiely within Python. I first web scrapped data from the Bureau of Economic Analysis API. The dataset I used was Real GDP of each State from the years : 1998, 2002, 2007, 2012, 2017, and 2012 - in chained 2012 Dollars value. After that I created a moving average time series model to predict GDP values for 2027. If you look at the line 
charts in the 'charts.ipynb' file, you can see that the GDP growth is predicted to be crazy by 2027. In reality that is probably not going to play out like that, unless in my opinion one of the following happens : AI becomes extremely productive very fast, which maybe possibily, or there is a major breakthrough in green technology that allows major countries such
as the US and China to transistion their economies to green very fast and efficiently and makes production costs go way down. The reason the model predicted the GDP to be so highly valued is because in the time frames I used GDP techincally did not go down, as the time frames used missed the dot com bubble, Great Recession and the COVID-19 pandmeic. So to the 
model it only looks like GDP goes up, so if the data is increasing in every interval, eventually the values will increase exponentially according to the model. The point of this project was not really to predict values, but more about the web scraping part. The API only allowed 100 requests a minute, so I had to put each loop on a 5 second delay just to get one state, to make sure I would not get timed-out. I guess just think of the prediction part as a bonus, and I am in my very beginning stage of learning how to use machine learning models. A better way to predict GDP would have been to create a model of every single year since 1998 to 2022, via downloading the CSVs from the BEA website so the model would at least account
GDP can go down, even if temporarily.
